# Deep-learning


# MLP -Multilayer perceptron 
BackPropogation - Backward propogation of error

**The Back-Propagation Neural Network** is a feed-forward network with a quite simple arhitecture. The arhitecture of the network consists of an input layer, one or more hidden layers and an output layer. This type of network can distinguish data that is not linearly separable. We use error back-propagation algorithm to tune the network iterative.

**The error back-propagation algorithm consists of two big steps:**

Feeding forward the input from the database to the input layer than to the hidden layers and finally to the output layer.
Calculating the output error and feeding it backwards tuning the network's variables.


